# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /datamodule: base_datamodule.yaml
  # - override /litmodel: fvr_litmodel.yaml
  - override /callbacks: default.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

datamodule:
  train_batch_size: 64
  val_batch_size: 64
  test_batch_size: 1
  num_workers: 4
  pin_memory: False
  datasets:
    train: 
      _target_: src.datasets.vein_dataset.VeinDataset
      data_root: /media/user/Toshiba4T/Kevin/Data/fv_samples_150ep
      samples_per_class: 1
      transforms:
        - _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.RandomResizedCrop
              size: [64, 128]
              scale: [0.5, 1.0]
              ratio: [1.5, 2.5]
            - _target_: torchvision.transforms.RandomRotation
              degrees: 3
            # - _target_: torchvision.transforms.ColorJitter
            #   brightness: 0.7
            #   contrast: 0.7
            - _target_: torchvision.transforms.ToTensor
            - _target_: torchvision.transforms.Normalize
              mean: [0.5]
              std: [0.5]
        - _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.RandomResizedCrop
              size: [64, 128]
              scale: [0.5, 1.0]
              ratio: [1.5, 2.5]
            - _target_: torchvision.transforms.RandomRotation
              degrees: 3
            # - _target_: torchvision.transforms.ColorJitter
            #   brightness: 0.7
            #   contrast: 0.7
            - _target_: torchvision.transforms.ToTensor
            - _target_: torchvision.transforms.Normalize
              mean: [0.5]
              std: [0.5]
    val:
      # _target_: src.datasets.vein_dataset.VeinDataset
      # data_root: /media/user/Toshiba4T/Kevin/Data/FV-USM-processed
      # samples_per_class: 12
      # transforms:
      #   - _target_: torchvision.transforms.Compose
      #     transforms:
      #       - _target_: torchvision.transforms.ToTensor
      #       - _target_: torchvision.transforms.Normalize
      #         mean: [0.5]
      #         std: [0.5]
      _target_: src.datasets.fvusm_dataset.FVUSMDataset
      root: /media/user/Toshiba4T/Kevin/Data/FV-USM-processed
      sample_per_class: 12
      transforms:
        _target_: torchvision.transforms.Compose
        transforms:
          - _target_: torchvision.transforms.ToTensor
          - _target_: torchvision.transforms.Normalize
            mean: [0.5]
            std: [0.5]
      mode: test
      inter_aug: ''
    test: ~

litmodel:
  _target_: src.litmodels.SimCLRLitModel
  backbone:
    _target_: src.networks.backbones.resnet18
    pretrained: True
  head:
    _target_: src.networks.heads.NormLinearHead
    input: 512 # BasicConvBlock: 512 BottleneckBlock: 2048
    output: 246 # num_classes for FVUSM dataset
  temperature: 0.05

optimizer:
  type: SGD
  lr: 0.01
  weight_decay: 0.0

lr_scheduler:
  type: MultiStepLR
  milestones: [60]
  gamma: 0.1

trainer:
  min_epochs: 10
  max_epochs: 100 # num_repeats: 25
  max_steps: -1
  log_every_n_steps: 10
  num_sanity_val_steps: 0

  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 0.0

# model_checkpoint:
#   monitor: "train_loss"
